{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426cb684-f9b6-4d1c-8632-7d001042d8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55283fe-a83d-419c-9b02-9a35637d9540",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"Book_of_John_RUS_MANS.csv\", \"Gospel_Mark_RUS_MANS.csv\", \"Bible_UDM_RUS.csv\", \"DICTIONARY_MANS_RUS_2.csv\", \"DICTIONARY_MANS_RUS.csv\", \"train.csv\"]\n",
    "corpus_text = list(pd.read_csv(corpus[0])['mans'].values)\n",
    "corpus_text += list(pd.read_csv(corpus[1])['mans'].values)\n",
    "corpus_text += list(pd.read_csv(corpus[2])['udm'].values)\n",
    "corpus_text += list(pd.read_csv(corpus[3])['mans'].values)\n",
    "corpus_text += list(pd.read_csv(corpus[4])['mans'].values)\n",
    "corpus_text += list(pd.read_csv(corpus[5])['pr_target'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f7b313-de58-4ed3-8b45-beaa58fb9080",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open(\"corpus.txt\", 'w') as f:\n",
    "    for text in corpus_text:\n",
    "        if text is not None and not isinstance(text, float):\n",
    "            f.write(text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e723341-96b6-44d0-835a-c4c90a3c7b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import T5TokenizerFast\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import T5ForConditionalGeneration\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "# Initialize the tokenizer\n",
    "fast_tokenizer = T5TokenizerFast.from_pretrained('ai-forever/ruT5-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d012c51b-0287-4a3b-8357-c44a82777281",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors\n",
    "\n",
    "# Initialize a Byte-Pair Encoding tokenizer\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "# Set pre-tokenizer and post-processor (if needed)\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "tokenizer.decoder = decoders.BPEDecoder()\n",
    "\n",
    "# Prepare the tokenizer training\n",
    "trainer = trainers.BpeTrainer(vocab_size=15000, special_tokens=[\"<pad>\", \"<unk>\", \"<s>\", \"</s>\", \"<mask>\"])\n",
    "\n",
    "# Train the tokenizer on your new language data\n",
    "files = [\"corpus.txt\"]\n",
    "tokenizer.train(files, trainer)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save(\"new_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a95fe1-e749-4b01-8959-c6c9cb8adb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_vocab = set(fast_tokenizer.get_vocab().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebc0ef5-5d8c-4026-987c-36c707a82436",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_tokens = list(tokenizer.get_vocab().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daff51d8-423a-4154-96d4-d4a05542f332",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_to_add = [token for token in new_tokens if token not in existing_vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e149ea-22f5-4e1d-a8b9-709b659a6196",
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_tokenizer.add_tokens(tokens_to_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4a6ec0-3d57-47ce-9ec0-8a05043cb8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fast_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4679734-9b86-4b67-89fa-2700926b045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115ffbf4-ea05-4438-9920-0bec06b9154c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_tokenizer.save_pretrained(\"rut5token_mans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c194d588-ee98-4051-8d8c-8250445f7462",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0a1b7d-d910-41a9-800f-94dd687cadbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_text_filtered = []\n",
    "for text in corpus_text:\n",
    "    if text is not None and not isinstance(text, float) and len(text) > 10:\n",
    "        corpus_text_filtered.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55773bfc-8dd1-41c5-8fd2-839c751a2623",
   "metadata": {},
   "outputs": [],
   "source": [
    "set([type(corpus_text_filtered[0]) for i in range(len(corpus_text_filtered))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2aa0548-27da-4e24-ae97-ce178f5feb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_text_filtered[22681]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a353fb-10e7-4abf-917c-e8d3486b49f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import T5TokenizerFast, T5ForConditionalGeneration, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the dataset\n",
    "class SpanCorruptionDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, span_prob=0.15, span_length=5):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.span_prob = span_prob\n",
    "        self.span_length = span_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        try:\n",
    "            tokenized = self.tokenizer.encode_plus(text, return_tensors='pt', padding='max_length', max_length=256, truncation=True)\n",
    "        except:\n",
    "            print(\"!=: \", text, idx)\n",
    "            assert 1 == 0\n",
    "\n",
    "        input_ids = tokenized['input_ids'].squeeze()\n",
    "        attention_mask = tokenized['attention_mask'].squeeze()\n",
    "\n",
    "        # Create corrupted inputs\n",
    "        input_ids_corrupted = input_ids.clone()\n",
    "        labels = input_ids.clone()  # Labels are the same as input_ids initially\n",
    "\n",
    "        if random.random() < self.span_prob:\n",
    "            start_idx = random.randint(0, len(input_ids) - self.span_length)\n",
    "            end_idx = min(start_idx + self.span_length, len(input_ids))\n",
    "            \n",
    "            input_ids_corrupted[start_idx:end_idx] = self.tokenizer.convert_tokens_to_ids(f\"<extra_ids_{0}>\")#torch.Tensor([self.tokenizer.convert_tokens_to_ids(f\"<extra_ids_{i}>\") for i in range(end_idx-start_idx)])\n",
    "\n",
    "            # Set the labels to -100 where the span is corrupted, so they are ignored in loss computation\n",
    "            labels[start_idx:end_idx] = -100\n",
    "\n",
    "        return {'input_ids': input_ids_corrupted, 'attention_mask': attention_mask, 'labels': labels}\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = T5TokenizerFast.from_pretrained(\"ruT5token\")\n",
    "model = T5ForConditionalGeneration.from_pretrained('ruT5_3ep.pt')\n",
    "#model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = SpanCorruptionDataset(corpus_text_filtered, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Set up optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(dataloader) * 7)  # 3 epochs\n",
    "\n",
    "# Training loop\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "model.train()\n",
    "loss_hist = []\n",
    "for epoch in range(3, 10):\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(dataloader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    loss_hist.append(total_loss / len(dataloader))\n",
    "    model.save_pretrained(f'ruT5_ep{epoch+1}')\n",
    "    tokenizer.save_pretrained(f'ruT5tok_ep{epoch+1}')\n",
    "    print(f\"Epoch {epoch + 1}: Loss = {total_loss / len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe648b9-4fe5-4db4-9a9b-c66a0fcd217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('ruT5_3ep.pt')\n",
    "tokenizer.save_pretrained('ruT5token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7ea0b3-7a5f-4449-a2a0-c47c15c7b71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5TokenizerFast, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "import torch\n",
    "from datasets import load_metric\n",
    "import sacrebleu\n",
    "\n",
    "# Загрузка данных из CSV\n",
    "csv_file_path = \"train.csv\"\n",
    "df = pd.read_csv(csv_file_path)\n",
    "df_val = pd.read_csv('val.csv')\n",
    "# Инициализация токенизатора и модели\n",
    "tokenizer = T5TokenizerFast.from_pretrained(\"ruT5tok_ep7\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"ruT5_ep7\")\n",
    "\n",
    "# Создание пользовательского датасета\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, source_col, target_col, max_length=128):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.source_col = source_col\n",
    "        self.target_col = target_col\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        source_text = self.dataframe.iloc[idx][self.source_col]\n",
    "        target_text = self.dataframe.iloc[idx][self.target_col]\n",
    "        \n",
    "        source_encoding = self.tokenizer(source_text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors=\"pt\")\n",
    "        target_encoding = self.tokenizer(target_text, truncation=True, padding='max_length', max_length=self.max_length, return_tensors=\"pt\")\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.squeeze(source_encoding['input_ids']),\n",
    "            'attention_mask': torch.squeeze(source_encoding['attention_mask']),\n",
    "            'labels': torch.squeeze(target_encoding['input_ids'])\n",
    "        }\n",
    "\n",
    "# Создание объектов Dataset\n",
    "train_dataset = TranslationDataset(df, tokenizer, 'pr_source', 'pr_target')\n",
    "val_dataset = TranslationDataset(df_val, tokenizer, 'pr_source', 'pr_target')\n",
    "\n",
    "# Определение аргументов для тренировки\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=10,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "# Определение метрик для оценки\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions[0]\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    print(decoded_preds[:10])\n",
    "\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    print(decoded_labels[:10])\n",
    "    # Оценка BLEU\n",
    "    bleu_metric = load_metric(\"bleu\")\n",
    "\n",
    "    formatted_labels = [[label] for label in decoded_labels]\n",
    "\n",
    "    # Дополнительные проверки для отладки форматов\n",
    "    print(f\"Пример decoded_preds: {decoded_preds[:2]}\")  # Посмотреть первые 2 предсказания\n",
    "    print(f\"Пример formatted_labels: {formatted_labels[:2]}\")  # Посмотреть первые 2 референса\n",
    "\n",
    "    # Проверка типов данных перед расчетом метрик\n",
    "\n",
    "    # Считаем метрику BLEU\n",
    "    #bleu_result = bleu_metri.compute(predictions=decoded_preds, references=formatted_labels)\n",
    "    bleu_score = sacrebleu.corpus_bleu(decoded_preds, [decoded_labels])\n",
    "    # Оценка CHRF\n",
    "    chrf_metric = load_metric(\"chrf\")\n",
    "    chrf_score = chrf_metric.compute(predictions=[[[d]] for d in decoded_preds], references=[[[d]] for d in decoded_labels])\n",
    "    return {\"bleu\": bleu_score.score, \"chrf\": chrf_score['score']}\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    \"\"\"\n",
    "    Original Trainer may have a memory leak. \n",
    "    This is a workaround to avoid storing too many tensors that are not needed.\n",
    "    \"\"\"\n",
    "    pred_ids = torch.argmax(logits[0], dim=-1)\n",
    "    return pred_ids, labels\n",
    "\n",
    "# Инициализация Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics\n",
    ")\n",
    "\n",
    "# Запуск тренировки\n",
    "trainer.train()\n",
    "\n",
    "# Оценка модели\n",
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad449c49-4954-43c2-83db-89a51fc48b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(\"results/checkpoint-762\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4682f8fa-1467-4d98-8925-ace73039fdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3c3148-3ab5-4e96-95f4-d9c23785810b",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(5)\n",
    "random.randint(0,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162751da-d314-427e-afa7-dd65df39a8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465eee22-443f-4c30-98ad-6b9e129c64ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([df_train[[\"pr_target\", \"pr_source\"]], pd.read_csv(\"test.csv\")[[\"pr_target\", \"pr_source\"]]])\n",
    "df_train.columns = [\"mans\", \"rus\"]\n",
    "df_train = pd.concat([df_train, pd.read_csv(\"DICTIONARY_MANS_RUS.csv\")[[\"mans\", \"rus\"]], pd.read_csv(\"DICTIONARY_MANS_RUS_2.csv\")[[\"mans\", \"rus\"]], pd.read_csv(\"Gospel_Mark_RUS_MANS.csv\")[[\"mans\", \"rus\"]], pd.read_csv(\"Book_of_John_RUS_MANS.csv\")[[\"mans\", \"rus\"]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fe43d4-639f-4980-8572-8332c90791d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(\"train_mansi.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba8669a-4b57-49e6-b376-fd92d5a8a264",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89876c7a-3622-4928-8758-332d55111c00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.mlspace-trpipe]",
   "language": "python",
   "name": "conda-env-.mlspace-trpipe-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

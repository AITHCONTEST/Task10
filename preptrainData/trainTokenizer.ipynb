{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcecaad6-343b-491d-8039-7d12d60a0b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "896706d9-dd0e-434a-b05f-f06d43c8b826",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import SentencePieceBPETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a146623d-cbdd-4530-bf4c-980c965510ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_articles():\n",
    "    text = \"\"\n",
    "    for i in range(13, 25):\n",
    "        dt = pd.read_csv(f\"dataMans/LUIMA-SERIPOS_20{i}_RUS_MANS.csv\")\n",
    "        mans = dt.mans.tolist()\n",
    "        for item in mans:\n",
    "            text += item.replace(\"\\n\", \" \").strip() + \"<eos>\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e4a663b-75c7-4227-b9f4-cb8ca36cb284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_vocab():\n",
    "    vocab_1 = pd.read_csv(\"dataMans/DICTIONARY_MANS_RUS_new.csv\")\n",
    "    vocab_2 = pd.read_csv(\"dataMans/DICTIONARY_MANS_RUS_2.csv\")\n",
    "    vocab_2 = vocab_2.dropna(subset=[\"rus\"])\n",
    "    rus_translate = vocab_2.rus.tolist()\n",
    "    mans_translate = vocab_2.mans.tolist()\n",
    "    note_translate = vocab_2.note.tolist()\n",
    "    for i in range(len(rus_translate)):\n",
    "        if \"гл. прист.\" in rus_translate[i]:\n",
    "            # print(i)\n",
    "            rus_translate[i] = rus_translate[i].replace(\"гл. прист.\", note_translate[i])\n",
    "            note_translate[i] = \"\"\n",
    "        if \"гл. приставка\" in rus_translate[i]:\n",
    "            rus_translate[i] = rus_translate[i].replace(\n",
    "                \"гл. приставка\", note_translate[i]\n",
    "            )\n",
    "            note_translate[i] = \"\"\n",
    "        if \"гл.прист.\" in rus_translate[i]:\n",
    "            # print(i)\n",
    "            rus_translate[i] = rus_translate[i].replace(\"гл.прист.\", note_translate[i])\n",
    "            note_translate[i] = \"\"\n",
    "    for i in range(len(rus_translate)):\n",
    "        for n in [\"III.\", \"II.\", \"I.\", \"I\", \"II\", \"III\"]:\n",
    "            # print(i)\n",
    "            if n in rus_translate[i]:\n",
    "                # print(i)\n",
    "                rus_translate[i] = rus_translate[i].replace(n, \"\")\n",
    "            if not isinstance(note_translate[i], float):\n",
    "                # print(note_translate[i])\n",
    "                if n in note_translate[i]:\n",
    "                    note_translate[i] = note_translate[i].replace(n, \"\")\n",
    "        rus_translate[i] = rus_translate[i].strip()\n",
    "        if not isinstance(note_translate[i], float):\n",
    "            note_translate[i] = note_translate[i].strip()\n",
    "    bad = [\n",
    "        \"4)\",\n",
    "        \") 2)\",\n",
    "        \"),2)\",\n",
    "        \") ,2)\",\n",
    "        \") 3)\",\n",
    "        \");2)\",\n",
    "        \"); 2)\",\n",
    "        \"2)\",\n",
    "        \"),1)\",\n",
    "        \");\",\n",
    "        \")\",\n",
    "    ]\n",
    "    for i in range(len(note_translate)):\n",
    "        if isinstance(note_translate[i], float):\n",
    "            continue\n",
    "        clean_last = False\n",
    "        for n in bad:\n",
    "            if n in note_translate[i]:\n",
    "                # print(note_translate[i])\n",
    "                note_translate[i] = note_translate[i].replace(n, \",\")\n",
    "                clean_last = True\n",
    "        if clean_last:\n",
    "            # print(note_translate[i])\n",
    "            res = note_translate[i].split(\",\")[:-1]\n",
    "            note_translate[i] = []\n",
    "            for elem in res:\n",
    "                if elem != \" \" or elem != \"\":\n",
    "                    note_translate[i].append(elem)\n",
    "            note_translate[i] = (\", \".join(note_translate[i])).strip()\n",
    "            if note_translate[i][-1] == \",\":\n",
    "                note_translate[i] = note_translate[i][:-1]\n",
    "\n",
    "    vocab_2.rus = rus_translate\n",
    "    vocab_2.mans = mans_translate\n",
    "    vocab_2.note = note_translate\n",
    "\n",
    "    rus_translate = vocab_1.rus.tolist()\n",
    "    mans_translate = vocab_1.mans.tolist()\n",
    "    note_translate = vocab_1.note.tolist()\n",
    "\n",
    "    rus_translate[47] = \"один\"\n",
    "    note_translate[47] = \"акв хум - один мужчина\"\n",
    "\n",
    "    rus_translate[903] = \"ещё\"\n",
    "    note_translate[903] = \"неа̄сюм иӈыт ёхты - отец ещё не приехал\"\n",
    "\n",
    "    vocab_1.rus = rus_translate\n",
    "    vocab_1.mans = mans_translate\n",
    "    vocab_1.note = note_translate\n",
    "\n",
    "    vocab = pd.concat([vocab_1, vocab_2])\n",
    "\n",
    "    rus_translate = vocab.rus.tolist()\n",
    "    mans_translate = vocab.mans.tolist()\n",
    "    note_translate = vocab.note.tolist()\n",
    "\n",
    "    words = {}\n",
    "    for i in range(len(rus_translate)):\n",
    "        s = f\"{mans_translate[i]}@@@{rus_translate[i]}\"\n",
    "        if s not in words:\n",
    "            words[s] = []\n",
    "        if isinstance(note_translate[i], float):\n",
    "            continue\n",
    "        words[s].append(note_translate[i])\n",
    "\n",
    "    rus_translate = []\n",
    "    mans_translate = []\n",
    "    note_translate = []\n",
    "    for key in words:\n",
    "        m, r = key.split(\"@@@\")\n",
    "        mans_translate.append(m)\n",
    "    return \"<eos>\".join(mans_translate)\n",
    "    \n",
    "\n",
    "def read_other():\n",
    "    text = \"\"\n",
    "    df_komi = pd.read_csv(\"dataMans/komi-rus.csv\")\n",
    "    komi = df_komi[\"Коми язык\"].tolist()[:10000]\n",
    "    text += \"<eos>\".join(komi)\n",
    "    df_udmurt = pd.read_csv(\"dataMans/udmurt-rus.csv\")\n",
    "    udm = df_udmurt.udm.tolist()[:10000]\n",
    "    text += \"<eos>\".join(udm)\n",
    "    df_mhr = pd.read_csv(\"dataMans/mhr-rus.csv\")\n",
    "    df_mhr = df_mhr.dropna(subset=[\"mhr\"])\n",
    "    mhr = df_mhr.mhr.tolist()[:10000]\n",
    "    text += \"<eos>\".join(mhr)\n",
    "\n",
    "    df_text = pd.read_csv(\"dataMans/READ_LITER.csv\")\n",
    "    text += \"<eos>\".join(df_text.mans.tolist())\n",
    "\n",
    "    df_text = pd.read_csv(\"dataMans/Gospel_Mark_RUS_MANS.csv\")\n",
    "    text += \"<eos>\".join(df_text.mans.tolist())\n",
    "\n",
    "    df_text = pd.read_csv(\"dataMans/Book_of_John_RUS_MANS.csv\")\n",
    "    text += \"<eos>\".join(df_text.mans.tolist())\n",
    "\n",
    "    df_text = pd.read_csv(\"dataMans/Bible_UDM_RUS.csv\")\n",
    "    df_text = df_text.dropna(subset=[\"udm\"])\n",
    "    text += \"<eos>\".join(df_text.udm.tolist())\n",
    "\n",
    "    df_text = pd.read_csv(\"dataMans/train.csv\")\n",
    "    text += \"<eos>\".join(df_text.target.tolist())\n",
    "\n",
    "    df_text = pd.read_csv(\"dataMans/val.csv\")\n",
    "    text += \"<eos>\".join(df_text.target.tolist())\n",
    "\n",
    "    df_text = pd.read_csv(\"dataMans/test.csv\")\n",
    "    text += \"<eos>\".join(df_text.target.tolist())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7ddd21c-5163-472a-8f39-f6a22bbf1061",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = read_articles()\n",
    "text += read_vocab() + \"<eos>\"\n",
    "text += read_other()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43de917d-a558-4e2e-aa5d-04ea30e0f21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_cnt = Counter(text)\n",
    "required_chars = ''.join([\n",
    "    k for k, v in chars_cnt.most_common() \n",
    "    if v >= 3 and k not in ' '\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "115f6332-0d34-429d-942d-7a4d01670147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'атынсломврикуе̄пэāгхь,.яздōйoes<>щёӯӈ-Тӧш\\xadбАМючС\\xa0ъӥКжИ\\t«Н»:ХВОП–1цРф20ēЛ;ӣ9ДЕ?ЮӟГЯ!УЭ5—і34Бӵ876ӱ)Ш(Ф\"”ӝЗҥӮЁЩЧ[]ЙЦ+aĀЖVIЫ_CӢDO%І…rЬtӞ№Ӧiun/\\nlm*ŌXhcp¬wdӜyg“Ӵk\\uf512xbRSӰAv̈TPMӤЪ„f@NEBĒQzWLFGU°JKH‑ӇӑZ\\\\jöū‒~#Y=ӆqá’\\'$ў|әé−Ҥ'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "required_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aba5a7aa-a83a-46fa-b64b-5329fd759f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('data_all.txt', 'w') \n",
    "file.write(text) \n",
    "file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69c474a4-1759-4588-9dac-0fcdda404ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: data_all.txt\n",
      "  input_format: \n",
      "  model_prefix: spm_man\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 16000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 1\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 16768\n",
      "  num_threads: 20\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 128\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: атынсломврикуе̄пэāгхь,.яздōйoes<>щёӯӈ-Тӧш­бАМючС ъӥКжИ\t«Н»:ХВОП–1цРф20ēЛ;ӣ9ДЕ?ЮӟГЯ!УЭ5—і34Бӵ876ӱ)Ш(Ф\"”ӝЗҥӮЁЩЧ[]ЙЦ+aĀЖVIЫ_CӢDO%І…rЬtӞ№Ӧiun/\n",
      "lm*ŌXhcp¬wdӜyg“ӴkxbRSӰAv̈TPMӤЪ„f@NEBĒQzWLFGU°JKH‑ӇӑZ\\jöū‒~#Y=ӆqá’'$ў|әé���Ҥ\n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 2\n",
      "  bos_id: -1\n",
      "  eos_id: 1\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 0\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: data_all.txt\n",
      "trainer_interface.cc(380) LOG(WARNING) Found too long line (25448091 > 16768).\n",
      "trainer_interface.cc(382) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(383) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 257 sentences\n",
      "trainer_interface.cc(416) LOG(INFO) Skipped 23 too long sentences.\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=159130\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=213\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 257 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=87013\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 15526 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 257\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 7873\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 7873 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=5138 obj=17.1782 num_tokens=25577 num_tokens/piece=4.97801\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=4471 obj=13.7816 num_tokens=25615 num_tokens/piece=5.72914\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: spm_man.model\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Internal: src/trainer_interface.cc(662) [(trainer_spec_.vocab_size()) == (model_proto->pieces_size())] Vocabulary size too high (16000). Please set it to a value <= 4601.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m SPM_PREFIX \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspm_man\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# special_tokens = [\"<UNK>\"]\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mspm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSentencePieceTrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata_all.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSPM_PREFIX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 16K\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcharacter_coverage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_extremely_large_corpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_dummy_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_sentencepiece_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_sentence_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4192\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# special_tokens=special_tokens,\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43munk_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbos_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequired_chars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequired_chars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/sentencepiece/__init__.py:1047\u001b[0m, in \u001b[0;36mSentencePieceTrainer.Train\u001b[0;34m(arg, logstream, **kwargs)\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m   1045\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mTrain\u001b[39m(arg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, logstream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1046\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m _LogStream(ostream\u001b[38;5;241m=\u001b[39mlogstream):\n\u001b[0;32m-> 1047\u001b[0m     \u001b[43mSentencePieceTrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Train\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/sentencepiece/__init__.py:1040\u001b[0m, in \u001b[0;36mSentencePieceTrainer._Train\u001b[0;34m(arg, **kwargs)\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SentencePieceTrainer\u001b[38;5;241m.\u001b[39m_TrainFromMap2(new_kwargs, sentence_iterator)\n\u001b[1;32m   1039\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1040\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSentencePieceTrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_TrainFromMap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.11/site-packages/sentencepiece/__init__.py:985\u001b[0m, in \u001b[0;36mSentencePieceTrainer._TrainFromMap\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_TrainFromMap\u001b[39m(args):\n\u001b[0;32m--> 985\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_sentencepiece\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSentencePieceTrainer__TrainFromMap\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Internal: src/trainer_interface.cc(662) [(trainer_spec_.vocab_size()) == (model_proto->pieces_size())] Vocabulary size too high (16000). Please set it to a value <= 4601."
     ]
    }
   ],
   "source": [
    "SPM_PREFIX = 'spm_man'\n",
    "# special_tokens = [\"<UNK>\"]\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input='data_all.txt',\n",
    "    model_prefix=SPM_PREFIX,\n",
    "    vocab_size=16000,  # 16K\n",
    "    character_coverage = 1,\n",
    "    num_threads=20,\n",
    "    train_extremely_large_corpus=False,\n",
    "    add_dummy_prefix=False,\n",
    "    max_sentencepiece_length=128,\n",
    "    max_sentence_length=4192*4,\n",
    "    # special_tokens=special_tokens,\n",
    "    pad_id=0,\n",
    "    eos_id=1,\n",
    "    unk_id=2,\n",
    "    bos_id=-1,\n",
    "    required_chars=required_chars,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a55c504-cd60-474e-9e0c-83816c197b61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

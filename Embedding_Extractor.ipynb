{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install fasttext\n",
        "!pip install uralicNLP"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 929
        },
        "id": "syZpYagxY0_H",
        "outputId": "91c6668b-8226-402c-ee7d-6b85fa31bc70"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m655.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.13.5-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (71.0.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.26.4)\n",
            "Using cached pybind11-2.13.5-py3-none-any.whl (240 kB)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.3-cp310-cp310-linux_x86_64.whl size=4246560 sha256=e819fe3fb008a4875f05516179739731f083cff1764e3a025d2c287f921add5c\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/a2/00/81db54d3e6a8199b829d58e02cec2ddb20ce3e59fad8d3c92a\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.3 pybind11-2.13.5\n",
            "Collecting uralicNLP\n",
            "  Downloading uralicNLP-1.5.2-py2.py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from uralicNLP) (2.32.3)\n",
            "Collecting mikatools>=0.0.6 (from uralicNLP)\n",
            "  Downloading mikatools-1.0.2-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting argparse (from uralicNLP)\n",
            "  Downloading argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: future>=0.18.2 in /usr/local/lib/python3.10/dist-packages (from uralicNLP) (1.0.0)\n",
            "Collecting tinydb (from uralicNLP)\n",
            "  Downloading tinydb-4.8.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting pyhfst (from uralicNLP)\n",
            "  Downloading pyhfst-1.3.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from mikatools>=0.0.6->uralicNLP) (4.66.5)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.10/dist-packages (from mikatools>=0.0.6->uralicNLP) (43.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->uralicNLP) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->uralicNLP) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->uralicNLP) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->uralicNLP) (2024.8.30)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography->mikatools>=0.0.6->uralicNLP) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography->mikatools>=0.0.6->uralicNLP) (2.22)\n",
            "Downloading uralicNLP-1.5.2-py2.py3-none-any.whl (111 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.9/111.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mikatools-1.0.2-py2.py3-none-any.whl (281 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.8/281.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Downloading pyhfst-1.3.0-py2.py3-none-any.whl (27 kB)\n",
            "Downloading tinydb-4.8.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: pyhfst, argparse, tinydb, mikatools, uralicNLP\n",
            "Successfully installed argparse-1.4.0 mikatools-1.0.2 pyhfst-1.3.0 tinydb-4.8.0 uralicNLP-1.5.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse"
                ]
              },
              "id": "8b3da2838c4e445f85aa554642d103fa"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AQp49xlA2lT8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import fasttext as ft\n",
        "from uralicNLP import uralicApi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "uralicApi.download(\"mns\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfJDocQedvJU",
        "outputId": "331fa17a-bc73-44c6-fe8a-485cb97d567e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading analyser for mns\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 3161/3161.1767578125 [00:00<00:00, 38629.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model analyser for mns was downloaded\n",
            "Downloading morpher-gt-desc.hfstol for mns\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 3906/3906.982421875 [00:00<00:00, 39932.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model morpher-gt-desc.hfstol for mns was downloaded\n",
            "Downloading analyzer.pt for mns\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 34420/34420.298828125 [00:00<00:00, 63132.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model analyzer.pt for mns was downloaded\n",
            "Downloading generator.pt for mns\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 34526/34526.8037109375 [00:00<00:00, 63140.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model generator.pt for mns was downloaded\n",
            "Downloading lemmatizer.pt for mns\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 34487/34487.197265625 [00:00<00:00, 62213.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model lemmatizer.pt for mns was downloaded\n",
            "Downloading analyser-norm for mns\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 2803/2803.2978515625 [00:00<00:00, 34339.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model analyser-norm for mns was downloaded\n",
            "Downloading analyser-dict for mns\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "10it [00:00, 10931.21it/s]                     \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model analyser-dict for mns was downloaded\n",
            "Downloading generator-desc for mns\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 2985/2985.9580078125 [00:00<00:00, 32364.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model generator-desc for mns was downloaded\n",
            "Downloading generator-norm for mns\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 2870/2870.86328125 [00:00<00:00, 36284.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model generator-norm for mns was downloaded\n",
            "Downloading generator for mns\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "10it [00:00, 14810.40it/s]                     \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model generator for mns was downloaded\n",
            "Downloading cg for mns\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 92%|█████████▏| 8/8.6689453125 [00:00<00:00, 15349.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model cg for mns was downloaded\n",
            "Downloading metadata.json for mns\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 86%|████████▌ | 1/1.1640625 [00:00<00:00, 3366.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model metadata.json for mns was downloaded\n",
            "Downloading dictionary.json for mns\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5737it [00:00, 66989.01it/s]                      "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model dictionary.json for mns was downloaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/train.csv\")[\"target\"]"
      ],
      "metadata": {
        "id": "yGIrzuJqZSUJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_symbols(text, f=True):\n",
        "    for symbol in \"!@#$%,.1234567890/?'\\\"\\\\«»_:;_…()”\":\n",
        "        text = text.replace(symbol, \" \")\n",
        "    first_words = set((\" \".join(text.lower().split())).split())\n",
        "\n",
        "    for symbol in \"!@#$%,.1234567890-/?'\\\"\\\\«»_-:;_…–()”\":\n",
        "        text = text.replace(symbol, \" \")\n",
        "    second_words = set((\" \".join(text.lower().split())).split())\n",
        "    return list(first_words | second_words)\n",
        "\n",
        "\n",
        "def lemmatize_mans_sentence(sentence):\n",
        "    sentence = replace_symbols(sentence)\n",
        "    words = []\n",
        "    for word in sentence:\n",
        "        norm_form = uralicApi.lemmatize(word, \"mns\")\n",
        "        if len(norm_form) == 0:\n",
        "            words.append(word)\n",
        "        else:\n",
        "            words.extend(sorted(uralicApi.lemmatize(word, \"mns\")))\n",
        "    return list(set(words) | set(sentence))\n"
      ],
      "metadata": {
        "id": "27TC2KQxa9w7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.apply(lemmatize_mans_sentence)"
      ],
      "metadata": {
        "id": "H6CJ8NT8bVTI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.apply(\" \".join)"
      ],
      "metadata": {
        "id": "ErfhKrNFg57h"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.savetxt(r'/content/train.txt', df.values, \"%s\")"
      ],
      "metadata": {
        "id": "QzPRoFzRg3JG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ft.train_unsupervised('/content/train.txt')"
      ],
      "metadata": {
        "id": "7JuXblLdZ6Mq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_nearest_neighbors(\"школат\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhrhqFglkDye",
        "outputId": "70b147ee-da4b-4f87-a6b1-154c3aaae79c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.9603418707847595, 'школа'),\n",
              " (0.948723554611206, 'школатт'),\n",
              " (0.9435409307479858, 'школавт'),\n",
              " (0.9430932402610779, 'школав'),\n",
              " (0.9395305514335632, 'ишколат'),\n",
              " (0.9335001111030579, 'школатыт'),\n",
              " (0.9147893786430359, 'школант'),\n",
              " (0.903683602809906, 'исколат'),\n",
              " (0.8910618424415588, 'школатын'),\n",
              " (0.8715890645980835, 'школа-интернатт')]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentence_embeddings(model, df):\n",
        "    embeddings = []\n",
        "    for sentence in df:\n",
        "        sentence = sentence.split()\n",
        "        sentence_embedding = model.get_word_vector(sentence[0])\n",
        "        for word in sentence[1:]:\n",
        "            sentence_embedding = np.add(model.get_word_vector(word), sentence_embedding)\n",
        "        sentence_embedding = sentence_embedding / len(sentence)\n",
        "    embeddings.append(sentence_embedding)\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "wTSb97iDfYBx"
      },
      "execution_count": 25,
      "outputs": []
    }
  ]
}